\chapter{Architecture}
In this section we explain the overall architecture of HBase and our the proposal for our solution.
\label{ch:architecture}

\begin{quotation}
{\small\it}

{\small\it }
\end{quotation}

\section{Extensions to the HBase internal mechanisms:}
In this section we outline the main internal mechanisms proposed to include into HBase to rule updates selectively during replication. First we introduce the core architecture of HBase. Following, up to the evaluation, we delve into the proposed additions in order to verify the feasibility of the implementation as well as what scenarios are best suited to our definition of consistency.

The initial approach follows built-in properties of HBase in regards to HDFS. We use the WALEdit data structure of Hbase rather than reinventing the wheel. That ensures a robust as well as a less prone to error implementation of the QoD extensions (Algorithm 1) A WALEdit structure contains information about the incoming updates to the tables in the system and it is later saved in the form of HLog entry, in that write ahead log as it needs to be be committed to persistent storage later, HDFS.

\section{Prospective evaluation and roadmap:}
In distributed scenarios, Facebook is currently using HBase to manage very large number of messages across data centers for their users, and not Cassandra which was built by them actualy. That is because of the simplicity of consistency model, as well as the ability of HBase to handle both a short set of volatile data and an ever-growing amount, that rarely gets accessed more than once. More specifically, in their architecture reports, a Key for each element is the userID as RowKey, word as Colum and messageID as Version and finally the value like offset of word in message (Data is sorted as: <userId, word, messageID>). That implicitly means that searching for the top messageIDs of an specific user and word is easily supported, and therefore queries run faster in the backend.

Hbase does that by populating and sorting a custom priority queue of items to be replicated until at a later stage a thread is triggered to pick up one at a time and then copy the relevant entries into another queue that will ship the rows to the remote location with the usual HBase mechanism.

In this work we assume the need for having a tailored replication mechanism that targets applications which require finer levels of consistency, others have previously used Snapshot Isolation techniques or as COPS the ALPS properties. In a previous paper, a.k.a as the conit consistency model from Duke University \cite{Duke:2001}, the system is mainly focused on generality not practicality. We prefer the later because we find it more rewarding to users that need to integrate a fully functional system with a replication framework that bests optimizes geo-replication costs. Actually there is an opened issue on the HBase community site for this particular matter \cite{JIRA-1}. 

In distributed clusters Facebook is currently using HBase to manage the messaging information across data centers not Cassandra~\cite{FacebookHBase}, that is because of the simplicity of consistency model as well as the ability of HBase to handle both a short set of volatile data and an ever-growing data set that rarely gets accessed more than once. More specifically, in their architecutre reports, a Key for each element is the userID as RowKey, word as Colum and messageID as Version and finally the value like offset of word in message (Data is sorted as: <userId, word, messageID>). That implicitly means that searching for the top messageIDs of an specific user and word is easily supported, and therefore queries can run faster in the backend. 

So, for filtering purposes, with our new proposal and implementation, that could directly enable administrators of the clusters to create quality-of-data policies that can analyze fetched data  isrt by inspecting some given bounds or semantics, and then receiving them on the master server at the other end of the replication chain if a match occurs. At first, we will be enhancing the eventual consistency model for inter-site replication in HBase by using an adaptive consistency model based on Service Level Objectives agreed or defined. The idea can be somehow similar to the "pluggable replication framework" proposed within the HBase community [ref here], so our work is two-fold purpose. First contributing to the open source community of HBase but also extending it with some extra capabilities, for that later we will implement batching of updates within our QoD framework.

With eventual consistency, updates and insertions are propagated asynchronously between clusters so Zookeeper is used for storing their positions in log files that hold the next log entry to be shipped in Hbase. To ensure cyclic replication (master to master) and prevent from copying same data back to the source, a sink location with remote procedure calls invoked is already into place with HBase. Therefore if we can control the edits to be shipped, we can also decide what is replicated, when or in other words how often. Keeping that in mind, we leverage the internal mechanisms of VFC$^{3}$ to tune HBase consistency, without requiring intrusion to the data schema and avoiding middleware overhead.

%% Build up here on WAL, ReplicationSource, etc. Also architecture drawings? space?
In order to provide bounded consistency guarantees with QoD, we add it to the inner workings of Hbase. There are existing command line tools as CopyTable in HBase where one can manually define what is going to be replayed to the log and this is useful for cases where new replicas need to be put up to date or in disaster recovery too. We focus our implementation efforts into organising a list of items in memory (extending the original structure reflected for the updates to be shipped), where we can apply our QoD principles and directly enforce constraints. We do that by defining our bounded model over data which is indexed and queried by key (containerId), and can be enforced through time constraints (T), sequence (number of pending updates) and value ( percentage of changes). For the prototype just sequence. In other words \texttt{HBaseQoD.enforce(containerId)}.

% Keep building on it here. Talk about the WALPlayer and possibly how to replay messages but not only.
Every new update is checked for QoD and shipped for replication, or retained for later and immediate replication at the moment of reaching the QoD triggering condition defined at runtime by the developer. The QoD allows for entries to be evaluated by one or several of the three parameters as seen in vector field consistency \emph{K (time, sequence value)}~\cite{Santos:2007} Any new updates over previous ones (same data) can be also checked for number of pending updates or value difference from previously replicated update, and then shipped or kept on the data structure accordingly.

% High-level algorithm we use to modify queued items for replication
\begin{algorithm*}
\caption{QoD high-level algorithm for filtering updates}\label{algo1}
\label{alg1}
\begin{algorithmic}[1]
\REQUIRE $containerId$
\ENSURE $maxBound \neq 0$ and $controlBound \neq 0$
%\STATE $y \leftarrow 1$
\WHILE{$enforceQoD (containerId)$}
\IF{$containerMaxK = 0$}
\RETURN $true$
\ELSE[$getactualK(containerID)$]
\STATE $actualK \leftarrow actualK.SEQ+1$
	\IF{$actualK.SEQ \geq containerMaxK.SEQ$}
	\STATE $actualK.SEQ \leftarrow 0$
	\RETURN $true$
	\ELSE
	\RETURN $false$
	\ENDIF
\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm*}